# RL_Learn
强化学习的灵感来自于心理学上的行为主义学科。强化学习的目标就是让个体在与环境交互过程中获得尽可能多的累计奖励。


这种"试错"学习方式，与"最优控制"的理念不谋而合。
"最优控制"的目标是使得动态系统随时间变化的某种度量最大化或者最小化。从控制的角度，试错学习的一系列动作实际上是时序决策的结果。
贝尔曼于1957年提出了最优控制问题的离散随机版本，并用马尔可夫决策过程(Markov decisionprocess,MDP )进行形式化描述。
此后，Ronald Howard在1960年开发出了MDP的策略迭代方法，为现代强化学习理论和算法打下了坚实的基础。


处理马尔可夫决策过程有三类基本方法:
动态规划(dynamic programming, DP):动态规划方法具有严格而清晰的数学基础且已经被深入研究，但它需要完整而精确的环境模型。这里的环境模型，简称模型，一般指环境的状态转移函数:在当前状态下智能体执性某个动作后，环境如何转移到下一个状态。
蒙特卡洛方法(Monte Carlo, MC):蒙特卡洛方法和时序差分(Temporal difference,TD)都不需要环境模型，属于无模型强化学习方法。蒙特卡洛方法在环境交互数据充分的条件下能够准确估计状态和动作的价值，从而收敛到有效的策略，但难以应用一步一步的增量式更新计算方式
时序差分(temporal difference, TD):时序差分方法用前后状态收益估计的差分来驱动价值函数的更新，能够增量式地更新，但在稳定性上有所欠缺。


